import json
import multiprocessing
from typing import Collection, Any
import http.client
from implementation import funsearch
from implementation import config
from implementation import sampler
from implementation import evaluator_accelerate
from implementation import evaluator
from implementation import code_manipulation

import json
import multiprocessing
from typing import Collection, Any
import http.client
from implementation import sampler

import TSP_utils_train
TSP_datasets = {'TSPLIB': TSP_utils_train.datasets['TSPLIB']}

def _trim_preface_of_body(sample: str) -> str:
    """Trim the redundant descriptions/symbols/'def' declaration before the function body.
    Please see my comments in sampler.LLM (in sampler.py).
    Since the LLM used in this file is not a pure code completion LLM, this trim function is required.

    -Example sample (function & description generated by LLM):
    -------------------------------------
    This is the optimized function ...
    def priority_v2(...) -> ...:
        return ...
    This function aims to ...
    -------------------------------------
    -This function removes the description above the function's signature, and the function's signature.
    -The indent of the code is preserved.
    -Return of this function:
    -------------------------------------
        return ...
    This function aims to ...
    -------------------------------------
    """
    lines = sample.splitlines()
    func_body_lineno = 0
    find_def_declaration = False
    for lineno, line in enumerate(lines):
        # find the first 'def' statement in the given code
        if line[:3] == 'def':
            func_body_lineno = lineno
            find_def_declaration = True
            break
    if find_def_declaration:
        code = ''
        for line in lines[func_body_lineno + 1:]:
            code += line + '\n'
        return code
    return sample


class LLMAPI(sampler.LLM):
    """Language model that predicts continuation of provided source code.
    """

    def __init__(self, samples_per_prompt: int, trim=True):
        super().__init__(samples_per_prompt)
        additional_prompt = ('Complete a different and more complex Python function. '
                             'Be creative and you can insert multiple if-else and for-loop in the code logic.'
                             'Only output the Python code, no descriptions.')
        self._additional_prompt = additional_prompt
        self._trim = trim

    def draw_samples(self, prompt: str) -> Collection[str]:
        """Returns multiple predicted continuations of `prompt`."""
        return [self._draw_sample(prompt) for _ in range(self._samples_per_prompt)]

    def _draw_sample(self, content: str) -> str:
        prompt = '\n'.join([content, self._additional_prompt])
        while True:
            try:
                conn = http.client.HTTPSConnection("api.chatanywhere.com.cn")
                payload = json.dumps({
                    "max_tokens": 512,
                    "model": "gpt-3.5-turbo",
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ]
                })
                headers = {
                    'Authorization': 'Bearer sk-iTq4EcFoWMGWtQDkkYILXCosM8RZOcxaBd54NQQRy87Pap1T',
                    'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',
                    'Content-Type': 'application/json'
                }
                conn.request("POST", "/v1/chat/completions", payload, headers)
                res = conn.getresponse()
                data = res.read().decode("utf-8")
                data = json.loads(data)
                response = data['choices'][0]['message']['content']
                # trim function
                if self._trim:
                    response = _trim_preface_of_body(response)
                return response
            except Exception:
                continue


class Sandbox(evaluator.Sandbox):
    """Sandbox for executing generated code. Implemented by RZ.

    RZ: Sandbox returns the 'score' of the program and:
    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).
    2) stops the execution of the code in time (avoid endless loop).
    """

    def __init__(self, verbose=False, numba_accelerate=False):
        """
        Args:
            verbose         : Print evaluate information.
            numba_accelerate: Use numba to accelerate the evaluation. It should be noted that not all numpy functions
                              support numba acceleration, such as np.piecewise().
        """
        self._verbose = verbose
        self._numba_accelerate = numba_accelerate

    def run(
            self,
            program: str,
            function_to_run: str,  # RZ: refers to the name of the function to run (e.g., 'evaluate')
            function_to_evolve: str,  # RZ: accelerate the code by decorating @numba.jit() on function_to_evolve.
            inputs: Any,  # refers to the dataset
            test_input: str,  # refers to the current instance
            timeout_seconds: int,
            **kwargs  # RZ: add this
    ) -> tuple[Any, bool]:
        """Returns `function_to_run(test_input)` and whether execution succeeded.

        RZ: If the generated code (generated by LLM) is executed successfully,
        the output of this function is the score of a given program.
        RZ: PLEASE NOTE THAT this SandBox is only designed for bin-packing problem.
        """
        dataset = inputs[test_input]
        result_queue = multiprocessing.Queue()
        process = multiprocessing.Process(
            target=self._compile_and_run_function,
            args=(program, function_to_run, function_to_evolve, dataset, self._numba_accelerate, result_queue)
        )
        process.start()
        process.join(timeout=timeout_seconds)
        if process.is_alive():
            # if the process is not finished in time, we consider the program illegal
            process.terminate()
            process.join()
            results = None, False
        else:
            if not result_queue.empty():
                results = result_queue.get_nowait()
            else:
                results = None, False

        if self._verbose:
            print(f'================= Evaluated Program =================')
            program_: code_manipulation.Program = code_manipulation.text_to_program(text=program)
            func_to_evolve_: str = kwargs.get('func_to_evolve', 'priority')
            function_: code_manipulation.Function = program_.get_function(func_to_evolve_)
            function_: str = str(function_).strip('\n')
            print(f'{function_}')
            print(f'-----------------------------------------------------')
            print(f'Score: {str(results)}')
            print(f'=====================================================')
            print(f'\n\n')

        return results

    def _compile_and_run_function(self, program, function_to_run, function_to_evolve, dataset, numba_accelerate,
                                  result_queue):
        try:
            # optimize the code (decorate function_to_run with @numba.jit())
            if numba_accelerate:
                program = evaluator_accelerate.add_numba_decorator(
                    program=program,
                    function_to_evolve=function_to_evolve
                )
            # compile the program, and maps the global func/var/class name to its address
            all_globals_namespace = {}
            # execute the program, map func/var/class to global namespace
            exec(program, all_globals_namespace)
            # get the pointer of 'function_to_run'
            function_to_run = all_globals_namespace[function_to_run]
            # return the execution results
            results = function_to_run(dataset)
            # the results must be int or float
            if not isinstance(results, (int, float)):
                result_queue.put((None, False))
                return
            result_queue.put((results, True))
        except:
            # if raise any exception, we assume the execution failed
            result_queue.put((None, False))


specification = r'''
import numpy as np
import tsplib95
import networkx
import itertools

def distance_matrix(problem):
    """Returns distance matrix.

    Args:
        TSP: TSP program datatype.

    Return:
        Matrix to which stores the distances between locations in TSP.
    """
    # convert into a networkx.Graph
    graph = problem.get_graph()
    # convert into a numpy distance matrix
    DM = networkx.to_numpy_matrix(graph)
    return DM

def tsp_tour_length(tour, distances):
    """Calculates the total length of the TSP tour."""
    distance = 0
    for i in range(1,len(tour)-1):
        current = tour[i-1]
        next = tour[i]
        distance += distances[current-1, next-1]
    return distance

@funsearch.run
def evaluate(instances: dict) -> float:
    """Evaluate heuristic function on a set of TSP instances."""
    # List storing tour lengths for each instance.
    tour_lengths = []
    # Solve TSP for each instance.
    for name in instances:
        instance = instances[name]
        # Parse the problem
        problem = tsplib95.parse(instance)
        # Get the distance matrix
        distances = distance_matrix(problem)
        # Get a list of cities.
        cities = list(range(1, len(distances)+1))  # Start from city 1
        # Create a tour based on the priorities.
        tour = [1]  # Start tour at city 1
        cities.remove(1)
        while len(cities) != 0:
            # Get the next city to visit
            next_city = select_next_node(tour[-1], cities, distances)
            tour.append(next_city)
            cities.remove(next_city)
        tour.append(1)  # End tour at city 1
        # Calculate the tour length.
        tour_length = tsp_tour_length(tour, distances)
        tour_lengths.append(tour_length)
        print("------------------------------------------------------")
        print(f'Dataset: {name}')
        print(f'Length/Cost: {tour_length}')
        print(f'Tour: {tour}')
        print("------------------------------------------------------")
    # Score of heuristic function is the average tour length across instances
    # (as we want to minimize the tour length).
    return -np.mean(tour_lengths)

@funsearch.evolve
def select_next_node(current_node: int, unvisited_nodes: list, distance_matrix: np.ndarray) -> int:
    """Draft code using random weight to determine the next node to visit
    
    Args:
        current_node: The current node.
        unvisited_nodes: List of cities to be visited. The datatype is list.
        distance_matrix: Distance matrix representing the distances between TSP instance. The matrix starts from 0.

    Return:
        The next node to visit.
    """   
    c1, c2, c3 = 0.4, 0.3, 0.2  
    scores = {} 
    
    if (len(unvisited_nodes)==1):
        return (unvisited_nodes[0])

    for node in unvisited_nodes:
        all_distances = []
        for i in unvisited_nodes:
            if i != node:
                all_distances.append(distance_matrix[node-1,i-1])
        average_distance_to_unvisited = np.mean(all_distances)
        std_dev_distance_to_unvisited = np.std(all_distances)

        score = c1 * distance_matrix[current_node-1,node-1] - c2 * average_distance_to_unvisited + c3 * std_dev_distance_to_unvisited
        scores[node] = score 

    next_node = min(scores, key=scores.get) 
    return next_node
'''

# It should be noted that the if __name__ == '__main__' is required.
# Because the inner code uses multiprocess evaluation.
if __name__ == '__main__':
    class_config = config.ClassConfig(llm_class=LLMAPI, sandbox_class=Sandbox)
    config = config.Config(samples_per_prompt=4, evaluate_timeout_seconds=60)

    global_max_sample_num = 10  # if it is set to None, funsearch will execute an endless loop
    funsearch.main(
        specification=specification,
        inputs=TSP_datasets,
        config=config,
        max_sample_nums=global_max_sample_num,
        class_config=class_config,
        log_dir='logs/funsearch_llm_api',
    )
